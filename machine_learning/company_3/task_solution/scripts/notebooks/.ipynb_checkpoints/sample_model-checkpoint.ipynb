{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and splitting stratified sample\n",
    "see [prev notebook](https://github.com/woldemarg/ds_tests/blob/master/machine_learning/company_3/task_solution/scripts/notebooks/get_sample.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (pd.read_csv(\"https://raw.githubusercontent.com/woldemarg/ds_tests/master/machine_learning/company_3/task_solution/derived/sample.csv\")\n",
    "          .drop([\"id\"], axis=1))\n",
    "\n",
    "y_sample = sample[\"gb\"]\n",
    "X_sample = sample.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sample,\n",
    "                                                    y_sample,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1234,\n",
    "                                                    stratify=y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholds to drop unavailing features\n",
    "th_nans = 0.35            #share of nan's per column\n",
    "th_high_cardinality = 10  #num of categories percolumn\n",
    "th_low_variance = 0.1     #std of a given feature\n",
    "th_corr = 0.05            #correlation of a given feature with target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * detecting columns with more than 35% of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_nans = (X_train\n",
    "            .columns[X_train.isna().mean() > th_nans]\n",
    "            .tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * detecting categorical features with more than 10 categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = X_train.columns[X_train.columns.str.startswith(\"cat\")]\n",
    "\n",
    "X_train.loc[:, cats] = X_train.loc[:, cats].astype(str)\n",
    "X_test.loc[:, cats] = X_test.loc[:, cats].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_high_cardinality = (cats[X_train[cats]\n",
    "                              .nunique() > th_high_cardinality]\n",
    "                         .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_init = list(set(cols_nans) | set(cols_high_cardinality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(cols_to_drop_init, axis=1)\n",
    "X_test = X_test.drop(cols_to_drop_init, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * imputing missing valuea for the rest of the columns\n",
    "Both for train and test sets we use *means* for numbers and *modes* for strings calculated strictly on the **train** set to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_left = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "nums_left = X_train.select_dtypes(include=np.number).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:, cats_left] = (X_train.loc[:, cats_left]\n",
    "                             .fillna(X_train.loc[:, cats_left].mode()))\n",
    "\n",
    "X_test.loc[:, cats_left] = (X_test.loc[:, cats_left]\n",
    "                            .fillna(X_train.loc[:, cats_left].mode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:, nums_left] = (X_train.loc[: , nums_left]\n",
    "                             .fillna(X_train.loc[:, nums_left].mean()))\n",
    "\n",
    "X_test.loc[:, nums_left] = (X_test.loc[:, nums_left]\n",
    "                            .fillna(X_train.loc[:, nums_left].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.isna().sum().sum())\n",
    "print(X_test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * one-hot encoding categorical features with simple Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "\n",
    "X_train, X_test = X_train.align(X_test,\n",
    "                                join=\"left\",\n",
    "                                axis=1,\n",
    "                                fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * detecting fetures with low variance and low corretation with target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_low_std = (X_train\n",
    "                .columns[(X_train.std() < th_low_variance) &\n",
    "                         (X_train.columns != \"gb\")]\n",
    "                .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_low_corr = (X_train\n",
    "                .columns[(X_train.corr().abs()[\"gb\"] < th_corr) &\n",
    "                         (X_train.columns != \"gb\")]\n",
    "                .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_final = list(set(cols_low_std) | set(cols_low_corr))\n",
    "cols_to_drop_final.append(\"gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5364, 48)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.drop(cols_to_drop_final, axis=1)\n",
    "X_test = X_test.drop(cols_to_drop_final, axis=1)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have managed to reduce number of features to *48* and leave only those ofthem to be considered significant from pure *technical* point of view (since we have nor column description neither can guess their meaning from context) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and evaluating simple baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC on test: 0.8570376016260162\n",
      "Confusion matrix:\n",
      "[[1243   69]\n",
      " [   7   23]]\n"
     ]
    }
   ],
   "source": [
    "target = y_train.value_counts()\n",
    "spw = target[0] / target[1]\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=1234,\n",
    "                          objective=\"binary:logistic\",\n",
    "                          scale_pos_weight=spw,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"ROC-AUC on test: {}\".format(roc_auc_score(y_test, y_pred)))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the ROC-AUC score is only **0.86** (possibly we can improve it further with hyper-parameters tuning), the Confusion matrix is worthy of special attention. With a highly imbalanced data, we've succeeded to *correctly predict* most of the values from minor class, hence we really don't know the business cost of FN and FP errors.\n",
    "\n",
    "Now we can proceed with building a simple pipeline based on above-defined steps - see [next notebook](https://github.com/woldemarg/ds_tests/blob/master/machine_learning/company_3/task_solution/scripts/notebooks/model_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(xgb_model, \"machine_learning/company_3/task_solution/derived/base_model.sav\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
