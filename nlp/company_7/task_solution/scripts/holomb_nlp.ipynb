{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (Spyder)",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "holomb_nlp.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woldemarg/ds_tests/blob/master/nlp/company_7/task_solution/scripts/holomb_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfWZf4L-PBEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from spacy.pipeline import EntityRuler\n",
        "from keywords import keywords"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uv1bowdZi55",
        "colab_type": "text"
      },
      "source": [
        "Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT6jIK_cPBFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/data.csv.gz\",\n",
        "                 index_col=\"id\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFr9q1OOacFA",
        "colab_type": "text"
      },
      "source": [
        "As it said in a task description in order to get the input data we have to concatenate two parts of JSON the corresponding column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jj_7qQiPBF3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "96677c9e-bd50-4b5b-946f-79a193f0a6b3"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>email</th>\n",
              "      <th>json</th>\n",
              "      <th>title</th>\n",
              "      <th>first_name</th>\n",
              "      <th>last_name</th>\n",
              "      <th>academic_title</th>\n",
              "      <th>department</th>\n",
              "      <th>school</th>\n",
              "      <th>processed</th>\n",
              "      <th>created_at</th>\n",
              "      <th>updated_at</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.abac.edu/</td>\n",
              "      <td>vfenn@abac.edu</td>\n",
              "      <td>{\"left\": \" the winner. The number on each ball...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-09-16 11:37:24</td>\n",
              "      <td>2020-02-06 03:33:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.abac.edu/</td>\n",
              "      <td>bray@abac.edu</td>\n",
              "      <td>{\"left\": \"er person and can be purchased onlin...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-09-16 11:37:24</td>\n",
              "      <td>2020-02-06 03:33:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.abac.edu/</td>\n",
              "      <td>admissions@abac.edu</td>\n",
              "      <td>{\"left\": \"ty, Prince Automotive Group, Rotary ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-09-16 11:37:24</td>\n",
              "      <td>2020-02-06 03:33:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.abac.edu/</td>\n",
              "      <td>webmaster@abac.edu</td>\n",
              "      <td>{\"left\": \"mics\\nRegistrar\\nTranscript Request\\...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-09-16 11:37:24</td>\n",
              "      <td>2020-02-06 03:33:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>https://www.alu.edu/</td>\n",
              "      <td>admissions@alu.edu</td>\n",
              "      <td>{\"left\": \"Abraham Lincoln University &amp; Online ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-09-16 11:37:24</td>\n",
              "      <td>2020-02-06 03:33:24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      url  ...           updated_at\n",
              "id                         ...                     \n",
              "1   https://www.abac.edu/  ...  2020-02-06 03:33:24\n",
              "2   https://www.abac.edu/  ...  2020-02-06 03:33:24\n",
              "3   https://www.abac.edu/  ...  2020-02-06 03:33:24\n",
              "4   https://www.abac.edu/  ...  2020-02-06 03:33:24\n",
              "5    https://www.alu.edu/  ...  2020-02-06 03:33:24\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxO_801fPBGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strings = []\n",
        "\n",
        "for jsn in df[\"json\"]:\n",
        "    d = json.loads(jsn)\n",
        "    s = \"\".join(v.strip()\n",
        "                .replace(\"\\n\", \" \") #minor preprocessing\n",
        "                .replace(\"\\t\", \" \")\n",
        "                for v in d.values())\n",
        "    strings.append(s)\n",
        "\n",
        "s_strings = pd.Series(strings, index=df.index) #to save identification"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2sF4sGmtRk0",
        "colab_type": "text"
      },
      "source": [
        "So, one of our inputs looks like below. Usually, we should consider preprocessing our inputs in terms of replacing artefacts and removing stop-words. But in this particular case, it turned out that ill-considered and quick text preprocessing blunts the effectiveness of further named entities recognition step. E.g. this part of a string Boeingâ€\\x9d by Mark Camolett after removing odd characters and stop-words is tended to be recognised as a single PROPN-entity Boeing Mark Camolett. So, for now, I leave all the inputs as is.\n",
        "\n",
        "Also, it should be noted, that the above string includes two *names* and one *job* (hereinafter - \"academic\") title. Further, I'll show how I deal with this challenge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5ahMt2Fcdhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "53974fc9-30c2-4981-b323-cb4dd2863b81"
      },
      "source": [
        "s_strings[1]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the winner. The number on each ball will be associated with an individual that purchased chances to win. The grand prize is a check with a designated value equal to 50 per cent (nearest $10) of the numbered golf balls sold. To participate in the tournament or the ball drop event, interested persons can contact Fenn at (229) 391-5067, email her at, or register online at https://www.abac.edu/academics/sanr-classic/ . ### Baldwin Players Announce Cast for ABAC Fall Production September 3 2019 Baldwin Players Announce Cast for ABAC Fall Production TIFTONâ€”Baldwin Playersâ€™ Director Brian Ray has announced the cast for the theatre troupeâ€™s upcoming production of â€œBoeing, Boeingâ€\\x9d by Mark Camolett'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlocs6uGtuga",
        "colab_type": "text"
      },
      "source": [
        "For NER I've been testing several models and it seems that *_lg* performs slightly better, though it incorrectly recognised e.g. *Hoffman Estates* and *Wilshire Highway*, as well as other 3-5 words' combinations as proper names.\n",
        "\n",
        "That is why I add a step in *nlp_main* pipeline in order to correct labels, firstly, for entities, which do not have a form of *Xxxx (X.) Xxxx*, and, secondly, for entities that include common *nouns* (as *Wilshire Highway*). For the POS tagging seems to be highly sensitive to the context, I've come up with using nlp_helper to feed only *doc.ents[i].text* without any confusing context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_YSqeGDPBGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_main = spacy.load(\"en_core_web_lg\", disable=[\"tagger\", \"parser\"])\n",
        "\n",
        "#to tag POS for words within recognised named entity\n",
        "nlp_helper = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"]) "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFgZttiYPBHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_all_propn(st):\n",
        "    propns = []\n",
        "    for w in st.split(\" \"):\n",
        "        w_doc = nlp_helper(w)\n",
        "        for t in w_doc:\n",
        "            propns.append(t.pos_)\n",
        "    return all((el == \"PROPN\" for el in propns))\n",
        "\n",
        "\n",
        "def correct_person_entities(nlp_doc):\n",
        "    new_ents = []\n",
        "    for ent in nlp_doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            #name assumes to be in form of Xxxx Xxxx with/without X. inbetween\n",
        "            if (re.search(r\"^([A-Z][\\w]+\\s[A-Z]?\\.?\\s?[A-Z][\\w]+)$\", \n",
        "                          ent.text) and is_all_propn(ent.text)):\n",
        "                new_ents.append(ent)\n",
        "        else:\n",
        "            new_ents.append(ent)\n",
        "    nlp_doc.ents = new_ents\n",
        "    return nlp_doc"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd6BgV8ouJbs",
        "colab_type": "text"
      },
      "source": [
        "In a task description, it was suggested to use *Matcher* for matching *academic titles*, though I've found *EntityRuler* much more convenient for adding named entities based on pattern dictionaries. Otherwise, I see this problem solving by manually building several sets of n-grams (n for the number of words in the compound terms in keyword dictionary) and sequential looping through those sets for (fuzzy) matches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zptXnol3PBHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reconstructing dictionary in a form needed for EntityRuler \n",
        "patterns = []\n",
        "\n",
        "for k, v in keywords().items():\n",
        "    for s in v:\n",
        "        new = {}\n",
        "        new[\"label\"] = k\n",
        "        new[\"pattern\"] = [{\"LOWER\": w.lower()} for w in s.split(\" \")]\n",
        "        patterns.append(new)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTbuBZZQPBIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ruler = EntityRuler(nlp_main)\n",
        "ruler.add_patterns(patterns)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFCIY0_xPBIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_main.add_pipe(ruler, before=\"ner\")\n",
        "nlp_main.add_pipe(correct_person_entities, after=\"ner\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsosc9ZauWBi",
        "colab_type": "text"
      },
      "source": [
        "**Important note!** The task is understood as extracting data from strings containing BOTH person name and academic title. That is why partly available data from inputs was omitted.\n",
        "\n",
        "Coming back to cases when there are several (and presumably not equivalent) numbers of names and/titles per input string. I assume that name and title placed as closer to each other as possible have more chance to be considered interconnected. For that reason, I build a matrix of differences between recognized patterns and extract for a given input only those within a minimal distance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kexIe6QPBJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_ls = []\n",
        "\n",
        "for i, el in s_strings.items():\n",
        "    doc = nlp_main(el)\n",
        "    #only those with both name and title\n",
        "    if (any([n.label_ == \"academic_title\" for n in doc.ents]) and \n",
        "            any([n.label_ == \"PERSON\" for n in doc.ents])):\n",
        "\n",
        "        names = [n.text for n in doc.ents if n.label_ == \"PERSON\"]\n",
        "        nx = np.asarray([n.start_char for n\n",
        "                         in doc.ents if n.label_ == \"PERSON\"])\n",
        "        titles = [n.text for n\n",
        "                  in doc.ents if n.label_ == \"academic_title\"]\n",
        "        ty = np.asarray([n.start_char for n\n",
        "                         in doc.ents if n.label_ == \"academic_title\"])\n",
        "\n",
        "        diff_arr = np.abs(ty - nx[:, np.newaxis]) #matrix of distances\n",
        "        min_vals = np.where(diff_arr == np.amin(diff_arr))\n",
        "        indicies = list(zip(min_vals[0], min_vals[1])) #min value within matrix\n",
        "        \n",
        "        #restore identification\n",
        "        data_ls.append((i, names[indicies[0][0]], titles[indicies[0][1]])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C0JFaPePBJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df = pd.DataFrame(data_ls, columns=[\"id\", \"name\", \"academic_title\"])\n",
        "data_df.set_index(\"id\", inplace=True)\n",
        "\n",
        "\n",
        "def remove_middle_name(nn):\n",
        "    n = nn.split(\" \")\n",
        "    return \" \".join((n[0], n[-1]))\n",
        "\n",
        "\n",
        "data_df.loc[:, \"name\"] = data_df[\"name\"].map(remove_middle_name)\n",
        "data_df[[\"first_name\", \"last_name\"]] = data_df[\"name\"].str.split(expand=True)\n",
        "\n",
        "data_df.loc[:, \"academic_title\"] = data_df[\"academic_title\"].str.title()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPP-05GCPBJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.update(data_df)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVqZKzOLusox",
        "colab_type": "text"
      },
      "source": [
        "So far I was able to fill nearly 56% of the rows with missing data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtIHfaXUPBKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(1 - df[\"first_name\"].isna().mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pQD5WqwPBLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.reset_index(inplace=True)\n",
        "df.to_csv(\"/content/data_new.csv.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5-cZBlZwSyL",
        "colab_type": "text"
      },
      "source": [
        "Mesuring time for processing a single text parcel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xgxoJL6PBLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(line):\n",
        "    l_doc = nlp_main(line)\n",
        "    if (any([n.label_ == \"academic_title\" for n in l_doc.ents]) and\n",
        "            any([n.label_ == \"PERSON\" for n in l_doc.ents])):\n",
        "\n",
        "        names = [n.text for n in l_doc.ents if n.label_ == \"PERSON\"]\n",
        "        nx = np.asarray([n.start_char for n\n",
        "                         in l_doc.ents if n.label_ == \"PERSON\"])\n",
        "        titles = [n.text for n\n",
        "                  in l_doc.ents if n.label_ == \"academic_title\"]\n",
        "        ty = np.asarray([n.start_char for n\n",
        "                         in l_doc.ents if n.label_ == \"academic_title\"])\n",
        "\n",
        "        diff_arr = np.abs(ty - nx[:, np.newaxis])\n",
        "        min_vals = np.where(diff_arr == np.amin(diff_arr))\n",
        "        indicies = list(zip(min_vals[0], min_vals[1]))\n",
        "        return (names[indicies[0][0]], titles[indicies[0][1]])\n",
        "\n",
        "\n",
        "%timeit get_data(s_strings[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPIomAPtuvr_",
        "colab_type": "text"
      },
      "source": [
        "###What could be done differently:\n",
        "1. Consider a flexible strategy of text preprocessing (removing artefacts and stop-words)\n",
        "2. Consider using less heavy models\n",
        "3. Consider building n-grams for fuzzy matching (also testing third-part libraries like [PhuzzyMatcher](https://github.com/jackmen/PhuzzyMatcher)) or applying another advanced algorithms like Aho-Corasick\n",
        "4. Consider what is more efficient  docs = list(nlp.pipe(texts)) or iterating through each input\n",
        "5. Consider dealing with triple named entities in *correct_person_entities*\n"
      ]
    }
  ]
}