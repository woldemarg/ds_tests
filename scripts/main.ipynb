{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy import stats\n",
        "from functools import reduce\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "description = pd.read_csv(\"https://raw.githubusercontent.com/woldemarg/lightit_test/master/data/encoded/columns_description.csv\", index_col=0)\n",
        "description.reset_index(drop=True, inplace=True)\n",
        "\n",
        "holidays = pd.read_csv(\"https://raw.githubusercontent.com/woldemarg/lightit_test/master/data/encoded/holidays_Japan.csv\", parse_dates=[0])\n",
        "\n",
        "# events = pd.read_csv(\"https://raw.githubusercontent.com/woldemarg/lightit_test/master/data/encoded/events_Hokkaido.csv\")\n",
        "\n",
        "# weather = pd.read_csv(\"https://raw.githubusercontent.com/woldemarg/lightit_test/master/data/encoded/weather_Hokkaido.csv\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "jalan_dcols_to_parse = [5, 6, 7, 10]\n",
        "\n",
        "jalan = pd.read_csv(\"https://raw.githubusercontent.com/woldemarg/lightit_test/master/data/encoded/jalan_shinchitose.csv\",\n",
        "                    parse_dates=jalan_dcols_to_parse)\n",
        "\n",
        "rakuten_dcols_to_parse = [4, 6, 16, 19]\n",
        "\n",
        "rakuten = pd.read_csv(\"https://raw.githubusercontent.com/woldemarg/lightit_test/master/data/encoded/rakuten_shinchitose.csv\",\n",
        "                      parse_dates=rakuten_dcols_to_parse)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "jalan.columns = description.loc[48:93, \"Row(EN)\"]\n",
        "rakuten.columns = description.loc[:47, \"Row(EN)\"]\n",
        "\n",
        "cols_to_select = [\"company_name\",\n",
        "                  \"request_date_time\",\n",
        "                  \"pickup_date_time\",\n",
        "                  \"return_date_time\",\n",
        "                  \"cancellation_date_time\",\n",
        "                  \"is_cancelled\"]\n",
        "\n",
        "jalan = (jalan\n",
        "         .assign(company_name=\"jalan\")\n",
        "         .assign(is_cancelled=np.invert(np.isnat(jalan.cancellation_date_time))\n",
        "                 .astype(int))\n",
        "         .loc[:, cols_to_select])\n",
        "\n",
        "rakuten = (rakuten\n",
        "           .assign(company_name=\"rakuten\")\n",
        "           .assign(is_cancelled=np.invert(\n",
        "               np.isnat(rakuten.cancel_request_date_time)).astype(int))\n",
        "           .rename(columns={\n",
        "               \"cancel_request_date_time\": \"cancellation_date_time\"})\n",
        "           .loc[:, cols_to_select])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_date(row):\n",
        "    \"\"\"Get date from all date_time columns in a given row.\"\"\"\n",
        "    row.iloc[1:-1] = row.iloc[1:-1].dt.date\n",
        "    return row\n",
        "\n",
        "\n",
        "raw_data = (pd.concat([jalan, rakuten], axis=0)\n",
        "            .reset_index(drop=True)\n",
        "            .apply(extract_date, axis=1)\n",
        "            .rename(columns=(lambda x: x[:-5]\n",
        "                    if x.endswith(\"_time\") else x)))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = (raw_data.loc[raw_data.is_cancelled != 1, :]\n",
        "              .groupby([\"company_name\", \"pickup_date\"])\n",
        "              .agg(target=pd.NamedAgg(column=\"pickup_date\",\n",
        "                                      aggfunc=\"count\"))\n",
        "              #.pickup_date.agg(\"count\").to_frame(\"target\") #same as above\n",
        "              .reset_index())\n",
        "\n",
        "days_wo_rakuten = (model_data.groupby(\"pickup_date\")\n",
        "                   .filter(lambda g: all(g.company_name == \"jalan\"))\n",
        "                   .assign(company_name=\"rakuten\")\n",
        "                   .assign(target=0))\n",
        "\n",
        "days_wo_jalan = (model_data.groupby(\"pickup_date\")\n",
        "                 .filter(lambda g: all(g.company_name == \"rakuten\"))\n",
        "                 .assign(company_name=\"jalan\")\n",
        "                 .assign(target=0))\n",
        "\n",
        "seed = 1234\n",
        "\n",
        "model_data_full = (pd.concat([model_data,\n",
        "                              days_wo_rakuten,\n",
        "                              days_wo_jalan], axis=0)\n",
        "                   .sample(frac=1, axis=0, random_state=seed)\n",
        "                   .reset_index(drop=True))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "model_data = model_data_full[model_data_full.pickup_date < pd.Timestamp(2019,1,1)]\n",
        "\n",
        "# model_data.to_csv(\"derived/model_data_py.csv\", index=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "timeline_raw = pd.pivot_table(model_data,\n",
        "                              values=\"target\",\n",
        "                              index=[\"pickup_date\"],\n",
        "                              columns=[\"company_name\"])\n",
        "\n",
        "dates_index = pd.date_range(min(timeline_raw.index),\n",
        "                            max(timeline_raw.index),\n",
        "                            freq=\"D\")\n",
        "\n",
        "timeline_days = timeline_raw.reindex(dates_index, fill_value=0)\n",
        "\n",
        "timeline_weeks = (timeline_days\n",
        "                  .groupby(pd.PeriodIndex(timeline_days.index, freq=\"W\"))\n",
        "                  .sum())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def apply_polynomial(y, d):\n",
        "    poly = np.poly1d(np.polyfit(range(1, 53),\n",
        "                                y.groupby(y.index.week).mean(), d))\n",
        "    rmse = np.sqrt(((poly(y.index.week) - y) ** 2).mean())\n",
        "    return poly, rmse\n",
        "\n",
        "\n",
        "def remove_seasonality(series):\n",
        "    weeks = series.index.week\n",
        "    degree = 2\n",
        "    model, rmse = apply_polynomial(series, degree)\n",
        "\n",
        "    while True:\n",
        "        degree += 1\n",
        "        new_model, new_rmse = apply_polynomial(series, degree)\n",
        "        if rmse - new_rmse < 0.5:\n",
        "            best_model, best_rmse = new_model, new_rmse\n",
        "            break\n",
        "        rmse = new_rmse\n",
        "\n",
        "    s_component = best_model(series.index.week)\n",
        "    a_series = np.subtract(series.reset_index(drop=True), s_component)\n",
        "\n",
        "    return s_component, a_series\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "for i, s in enumerate([\"jalan\", \"rakuten\"], 1):\n",
        "    s_component, a_series = remove_seasonality(timeline_weeks[s])\n",
        "    plt.subplot(1, 2, i)\n",
        "    plt.plot(timeline_weeks[s].reset_index(drop=True))\n",
        "    plt.plot(s_component, color=\"red\")\n",
        "    plt.ylim(0, 70)\n",
        "    plt.title(s)\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rakuten_seasonality, rakuten_adjusted = remove_seasonality(timeline_weeks.rakuten)\n",
        "jalan_seasonality, jalan_adjusted = remove_seasonality(timeline_weeks.jalan)\n",
        "\n",
        "plt.figure()\n",
        "plt.boxplot([jalan_adjusted, rakuten_adjusted])\n",
        "plt.xticks([1, 2],[\"jalan\", \"rakuten\"])\n",
        "plt.title(\"Adjusted series\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_outliers(*series):\n",
        "    out = []\n",
        "    for s in series:\n",
        "        zscores = list(np.abs(stats.zscore(s)))\n",
        "        indicies = [i for i, v in enumerate(zscores) if v >= 2.5]\n",
        "        out.append(indicies)\n",
        "\n",
        "    return list(dict.fromkeys(reduce(lambda x, y: x + y, out)))\n",
        "\n",
        "\n",
        "out_periods = (timeline_weeks\n",
        "               .reset_index()\n",
        "               .iloc[get_outliers(jalan_adjusted,\n",
        "                                  rakuten_adjusted)][\"index\"]\n",
        "               .rename(\"period\"))\n",
        "\n",
        "format_to_year_week = lambda d: \"_\".join([str(d.year), str(d.week)])\n",
        "\n",
        "out_weeks = out_periods.map(format_to_year_week)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "format_to_year_week_2 = lambda d: \"_\".join([str(d.isocalendar()[0]),\n",
        "                                            str(d.isocalendar()[1])])\n",
        "\n",
        "model_data.loc[:, \"year_week\"] = (model_data.pickup_date\n",
        "                                  .apply(format_to_year_week_2))\n",
        "\n",
        "model_data = model_data.loc[~model_data.year_week.isin(out_weeks)]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def is_holidays_series(row):\n",
        "    if (row.Japan_prev == 0 and\n",
        "        row.Japan == 1 and\n",
        "        row.Japan_next == 1):\n",
        "        return 1\n",
        "    elif (row.wday == \"Fri\" and\n",
        "          row.Japan == 1):\n",
        "        return 1\n",
        "    elif (row.wday == \"Sat\" and\n",
        "          row.Japan_next_next == 1):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "holidays_mod = (holidays\n",
        "                .assign(Japan_prev=holidays.Japan.shift(1))\n",
        "                .assign(Japan_next=holidays.Japan.shift(-1))\n",
        "                .assign(Japan_next_next=holidays.Japan.shift(-2))\n",
        "                .assign(wday=holidays.day.apply(lambda d: d.strftime(\"%a\"))))\n",
        "\n",
        "holidays[\"is_holidays_series\"] = holidays_mod.apply(is_holidays_series,\n",
        "                                                    axis=1)\n",
        "holidays[\"day\"] = holidays.day.dt.date\n",
        "\n",
        "model_data = (model_data\n",
        "              .merge(holidays,\n",
        "                     how=\"left\",\n",
        "                     left_on=\"pickup_date\",\n",
        "                     right_on=\"day\")\n",
        "              .drop([\"day\", \"Japan\", \"year_week\"], axis=1))\n",
        "# %%\n",
        "train = (model_data\n",
        "         .assign(month=model_data.pickup_date.apply(lambda d: d.strftime(\"%b\")))\n",
        "         .assign(wday=model_data.pickup_date.apply(lambda d: d.strftime(\"%a\")))\n",
        "         .drop([\"pickup_date\"], axis=1))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = train.target\n",
        "X = train.drop([\"target\"], axis=1)\n",
        "\n",
        "categorical_cols = [cname for cname in X\n",
        "                    if X[cname].dtype == \"object\"]\n",
        "\n",
        "OH_encoder = OneHotEncoder(handle_unknown=\"error\",\n",
        "                           drop=\"first\", sparse=False)\n",
        "\n",
        "OH_cols = pd.DataFrame(OH_encoder\n",
        "                       .fit_transform(X[categorical_cols]))\n",
        "\n",
        "OH_cols_names = OH_encoder.get_feature_names(categorical_cols)\n",
        "OH_cols.columns = OH_cols_names\n",
        "\n",
        "num_X = X.drop(categorical_cols, axis=1)\n",
        "OH_X = pd.concat([num_X, OH_cols], axis=1)\n",
        "\n",
        "\n",
        "models = []\n",
        "models.append((\"Ridge\", Ridge(random_state=seed)))\n",
        "models.append((\"Lasso\", Lasso(random_state=seed)))\n",
        "models.append((\"KNN\", KNeighborsRegressor()))\n",
        "models.append((\"RF\", RandomForestRegressor(random_state=seed)))\n",
        "models.append((\"GBR\", GradientBoostingRegressor(random_state=seed)))\n",
        "models.append((\"SVR\", SVR()))\n",
        "\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "scoring = \"neg_root_mean_squared_error\"\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "    cv_results = -1 * cross_val_score(model,\n",
        "                                      OH_X,\n",
        "                                      y,\n",
        "                                      scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "\n",
        "\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Algorithm Comparison\")\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create an object of the RandomForestRegressor\n",
        "model_RF = RandomForestRegressor(random_state=seed)\n",
        "\n",
        "# fit the model with the training data\n",
        "rf_default = model_RF.fit(OH_X, y)\n",
        "\n",
        "\n",
        "feat_importances = pd.Series(rf_default.feature_importances_,\n",
        "                             index = OH_X.columns)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "feat_importances.nlargest(15).plot(kind=\"barh\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_alt = (train\n",
        "             .assign(is_weekend=(train.wday.isin([\"Sat\", \"Sun\"])) * 1)\n",
        "             .drop([\"wday\"], axis=1))\n",
        "\n",
        "X_alt = train_alt.drop([\"target\"], axis=1)\n",
        "\n",
        "categorical_cols_alt = [cname for cname in X_alt\n",
        "                        if X_alt[cname].dtype == \"object\"]\n",
        "\n",
        "OH_cols_alt = pd.DataFrame(OH_encoder\n",
        "                           .fit_transform(X_alt[categorical_cols_alt]))\n",
        "\n",
        "OH_cols_names_alt = OH_encoder.get_feature_names(categorical_cols_alt)\n",
        "OH_cols_alt.columns = OH_cols_names_alt\n",
        "\n",
        "num_X_alt = X_alt.drop(categorical_cols_alt, axis=1)\n",
        "OH_X_alt = pd.concat([num_X_alt, OH_cols_alt], axis=1)\n",
        "\n",
        "scores_alt = -1 * cross_val_score(model_RF,\n",
        "                                  OH_X_alt,\n",
        "                                  y,\n",
        "                                  scoring=scoring)\n",
        "\n",
        "msg = \"RF_alt: %f (%f)\" % (scores_alt.mean(), scores_alt.std())\n",
        "print(msg)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_sizes = [1, 250, 500, 750, 1000, 1250, 1443]\n",
        "\n",
        "\n",
        "def compare_learning_curves(predictors,\n",
        "                            target,\n",
        "                            train_sizes,\n",
        "                            cv):\n",
        "    train_sizes, train_scores, validation_scores = learning_curve(\n",
        "        model_RF,\n",
        "        predictors,\n",
        "        target,\n",
        "        train_sizes=train_sizes,\n",
        "        cv=5,\n",
        "        scoring=\"neg_root_mean_squared_error\")\n",
        "\n",
        "    train_scores_mean = -train_scores.mean(axis=1)\n",
        "    validation_scores_mean = -validation_scores.mean(axis=1)\n",
        "\n",
        "    plt.plot(train_sizes,\n",
        "             train_scores_mean,\n",
        "             label=\"train RMSE\")\n",
        "    plt.plot(train_sizes,\n",
        "             validation_scores_mean,\n",
        "             label=\"test RMSE\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Training set size\")\n",
        "    plt.ylim(0, 3)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "for i, data in enumerate([OH_X, OH_X_alt], 1):\n",
        "    plt.subplot(1, 2, i)\n",
        "    compare_learning_curves(data, y, train_sizes, 5)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
        "\n",
        "# Number of features to consider at every split\n",
        "max_features = [\"auto\", \"sqrt\"]\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
        "max_depth.append(None)\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {\"n_estimators\": n_estimators,\n",
        "               \"max_features\": max_features,\n",
        "               \"max_depth\": max_depth,\n",
        "               \"min_samples_split\": min_samples_split,\n",
        "               \"min_samples_leaf\": min_samples_leaf,\n",
        "               \"bootstrap\": bootstrap}\n",
        "\n",
        "# Random search of parameters, using 3 fold cross validation,\n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator=model_RF,\n",
        "                               param_distributions=random_grid,\n",
        "                               n_iter=100,\n",
        "                               cv=3,\n",
        "                               verbose=2,\n",
        "                               random_state=seed,\n",
        "                               n_jobs=-1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(OH_X_alt, y)\n",
        "\n",
        "print(rf_random.best_params_)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create the parameter grid based on the results of random search\n",
        "param_grid = {\n",
        "    \"bootstrap\": [False],\n",
        "    \"max_depth\": [5, 10, 15],\n",
        "    \"max_features\": [3, 5, 7],\n",
        "    \"min_samples_leaf\": [3, 4, 5],\n",
        "    \"min_samples_split\": [3, 5, 10],\n",
        "    \"n_estimators\": [1500, 2000, 2500]\n",
        "}\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator=model_RF,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3,\n",
        "                           verbose=2,\n",
        "                           n_jobs=-1)\n",
        "\n",
        "grid_search.fit(OH_X_alt, y)\n",
        "\n",
        "grid_search.best_params_\n",
        "\n",
        "scores_tuned = -1 * cross_val_score(RandomForestRegressor(\n",
        "    n_estimators=2000,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=4,\n",
        "    max_features=4,\n",
        "    max_depth=10,\n",
        "    bootstrap=False,\n",
        "    random_state=1234),\n",
        "                             OH_X_alt,\n",
        "                             y,\n",
        "                             scoring=scoring)\n",
        "\n",
        "msg = \"RF_alt_tuned: %f (%f)\" % (scores_tuned.mean(), scores_tuned.std())\n",
        "print(msg)\n",
        "# %%\n",
        "pipe_data = (model_data\n",
        "             .drop([\"target\"], axis=1)\n",
        "             .set_index(\"pickup_date\"))\n",
        "\n",
        "\n",
        "def assign_month(X):\n",
        "    get_month = lambda i: i.strftime(\"%b\")\n",
        "    return X.assign(month=X.index.map(get_month))\n",
        "\n",
        "\n",
        "def assign_weekend(X):\n",
        "    get_wday_name = lambda i: i.strftime(\"%a\")\n",
        "    return X.assign(is_weekend=[1 if get_wday_name(i) in ([\"Sat\", \"Sun\"])\n",
        "                                else 0 for i in X.index])\n",
        "\n",
        "date_parser = Pipeline(steps=[\n",
        "    (\"get_month\", FunctionTransformer(assign_month)),\n",
        "    (\"get_weekend\", FunctionTransformer(assign_weekend))\n",
        "    ])\n",
        "\n",
        "\n",
        "col_processor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"one_hot\",\n",
        "         OneHotEncoder(handle_unknown=\"ignore\"),\n",
        "         [\"company_name\", \"month\", \"is_weekend\"])\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# final_model = RandomForestRegressor(n_estimators=2000,\n",
        "#                                     min_samples_split=5,\n",
        "#                                     min_samples_leaf=4,\n",
        "#                                     max_features=4,\n",
        "#                                     max_depth=10,\n",
        "#                                     bootstrap=False,\n",
        "#                                     random_state=1234)\n",
        "# =============================================================================\n",
        "\n",
        "# Bundle preprocessing and modeling code in a pipeline\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    (\"parse_date\", date_parser),\n",
        "    (\"pre_process\", col_processor),\n",
        "    (\"build_model\", model_RF)])\n",
        "\n",
        "\n",
        "#model_pipeline.fit(pipe_data, y)\n",
        "\n",
        "# predict target values on the training data\n",
        "#model_pipeline.predict(train_x)\n",
        "\n",
        "scores_tuned_3 = -1 * cross_val_score(model_pipeline,\n",
        "                             pipe_data,\n",
        "                             y,\n",
        "                             scoring=scoring)\n",
        "msg = \"RF_alt: %f (%f)\" % (scores_tuned_3.mean(), scores_tuned_3.std())\n",
        "print(msg)\n",
        "# %%\n",
        "february = pd.date_range(\"2019-01-01min(timeline_raw.index),\n",
        "                            max(timeline_raw.index),\n",
        "                            freq=\"D\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}